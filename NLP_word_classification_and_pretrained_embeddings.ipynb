{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyND7DkXfBtu43533zR7v9JS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Badisj/Recurrent-Neural-Nets-NLP/blob/main/NLP_word_classification_and_pretrained_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWsifb4bjVCw"
      },
      "source": [
        "## Text classification using Neural Networks\n",
        "\n",
        "The goal of this notebook is to use Neural Networks for text classification.\n",
        "\n",
        "In this notebook, we will:\n",
        "- Train a shallow model with learning embeddings\n",
        "- Download pre-trained embeddings from Glove\n",
        "- Use these pre-trained embeddings\n",
        "\n",
        "However keep in mind:\n",
        "- Deep Learning can be better on text classification that simpler ML techniques, but only on very large datasets and well designed/tuned models.\n",
        "- We won't be using the most efficient (in terms of computing) techniques, as Keras is good for prototyping but rather inefficient for training small embedding models on text.\n",
        "- The following projects can replicate similar word embedding models much more efficiently: [word2vec](https://github.com/dav/word2vec) and [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)   (self-supervised learning only), [fastText](https://github.com/facebookresearch/fastText) (both supervised and self-supervised learning), [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) (supervised learning).\n",
        "- Plain shallow sparse TF-IDF bigrams features without any embedding and Logistic Regression or Multinomial Naive Bayes is often competitive in small to medium datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmWbUjq7jeuh"
      },
      "source": [
        "## The BBC topic classification dataset\n",
        "\n",
        "The BBC provides some benchmark topic classification datasets in English at: http://mlg.ucd.ie/datasets/bbc.html.\n",
        "\n",
        "The raw text (encoded with the latin-1 character encoding) of the news can be downloaded as a ZIP archive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiNqi42jjTzq"
      },
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import zipfile\n",
        "try:\n",
        "    from urllib.request import urlretrieve\n",
        "except ImportError:\n",
        "    from urllib import urlretrieve\n",
        "\n",
        "\n",
        "BBC_DATASET_URL = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
        "zip_filename = BBC_DATASET_URL.rsplit('/', 1)[1]\n",
        "BBC_DATASET_FOLDER = 'bbc'\n",
        "if not op.exists(zip_filename):\n",
        "    print(\"Downloading %s to %s...\" % (BBC_DATASET_URL, zip_filename))\n",
        "    urlretrieve(BBC_DATASET_URL, zip_filename)\n",
        "\n",
        "if not op.exists(BBC_DATASET_FOLDER):\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as f:\n",
        "        print(\"Extracting contents of %s...\" % zip_filename)\n",
        "        f.extractall('.')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95TocJM8c2-"
      },
      "source": [
        "Each of the five folders contains text files from one of the five topics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2GwX_hfjOTB",
        "outputId": "bdb570ce-6bd2-4378-9f40-dddd03545990"
      },
      "source": [
        "target_names = sorted([folder for folder in os.listdir(BBC_DATASET_FOLDER)\n",
        "                       if op.isdir(op.join(BBC_DATASET_FOLDER, folder))])\n",
        "target_names"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['business', 'entertainment', 'politics', 'sport', 'tech']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzgXmAw18hQm"
      },
      "source": [
        "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtvdLrAIkG9k"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "targets, filenames = [], []\n",
        "for target, folder in enumerate(target_names):\n",
        "    class_path = op.join(BBC_DATASET_FOLDER, folder)\n",
        "    for item in sorted(os.listdir(class_path)):\n",
        "        filenames.append(op.join(class_path, item))\n",
        "        targets.append(target)\n",
        "\n",
        "targets_train, targets_test, filenames_train, filenames_test = train_test_split(\n",
        "    targets, filenames, test_size = 200)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MkNjozCmgbo",
        "outputId": "4a71c545-8e9d-4dba-c302-ccdfb352bc6a"
      },
      "source": [
        "len(targets_train), len(filenames_train)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2025, 2025)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlM2-YBimkbv",
        "outputId": "facf815a-5197-4d94-8102-963810096101"
      },
      "source": [
        "len(targets_test), len(filenames_test)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYkRWwZU8lLJ"
      },
      "source": [
        "Let's check that text of some document have been loaded correctly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP__7pgNmtSa",
        "outputId": "605d6bc8-1c21-4b82-e908-3b70cfa46ddc"
      },
      "source": [
        "idx = 234\n",
        "\n",
        "with open(filenames_train[idx], 'rb') as f:\n",
        "      print(target_names[targets_train[idx]], ':')\n",
        "      print()\n",
        "      print(f.read().decode('latin-1')[:500] + '...')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sport :\n",
            "\n",
            "Scotland v Italy (Sat)\n",
            "\n",
            "Murrayfield, Edinburgh\n",
            "\n",
            "Saturday, 26 February\n",
            "\n",
            "1400 GMT\n",
            "\n",
            "BBC1, Five Live and this website\n",
            "\n",
            "Victory for the Azzurri in Rome last year saw Scotland end their campaign without a victory. And the pressure is on Scotland coach Matt Williams as he seeks a first Six Nations victory at the eighth attempt. Italy have lost both their opening games at home to Ireland and Wales, but travel to Edinburgh with high hopes.\n",
            "\n",
            "Their coach John Kirwan has warned his side they must eradicate ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TI2cNdXooM1"
      },
      "source": [
        "texts_train = [open(fn,'rb').read().decode('latin-1') for fn in filenames_train]\n",
        "texts_test = [open(fn,'rb').read().decode('latin-1') for fn in filenames_test]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfW-Q5tB8plf"
      },
      "source": [
        "# A first baseline model\n",
        "For simple topic classification problems, one should always try a simple method first. In this case a good baseline is extracting TF-IDF normalized bag of bi-grams features and then use a simple linear classifier such as logistic regression.\n",
        "It's a very efficient method and should give us a strong baseline to compare our deep learning method against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWM2nbF9qD9Q"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "text_classifier = make_pipeline(\n",
        "                      TfidfVectorizer(max_df = .8, min_df = 3, ngram_range = (1,2)),\n",
        "                      LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKCuneCGrFG_",
        "outputId": "440ba868-a006-4b92-866d-b3ccd73b72d4"
      },
      "source": [
        "%time _ = text_classifier.fit(texts_train, targets_train)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.86 s, sys: 4.98 s, total: 12.8 s\n",
            "Wall time: 8.35 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtZoku7nrGY7",
        "outputId": "d5baaf28-0f55-4b45-8545-96cecccde1b1"
      },
      "source": [
        "text_classifier.score(texts_test, targets_test)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.97"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDApuwLLs99q"
      },
      "source": [
        "Very few classification errors on 200 test documents for a model fit in less than 10s. It's quite unlikely that we can significantly beat that baseline with a more complex deep learning based model. However let's try to reach a comparable level of accuracy with Embeddings-based models just for teaching purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoowS_NF86bS"
      },
      "source": [
        "### Preprocessing text for the (supervised) CBOW model\n",
        "\n",
        "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
        "\n",
        "The following cells uses Keras to preprocess text:\n",
        "- using a tokenizer. You may use different tokenizers (from scikit-learn, NLTK, custom Python function etc.). This converts the texts into sequences of indices representing the `20000` most frequent words\n",
        "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`)\n",
        "- we convert the output classes as 1-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPX1No83s9t1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "MOST_FREQUENT = 20000\n",
        "tokenizer = Tokenizer(num_words = MOST_FREQUENT)\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuyngRZS8_tw"
      },
      "source": [
        "Tokenized sequences are converted to list of token ids (with an integer code):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFgWEh9ErzCO",
        "outputId": "c5808d5c-7b94-46a0-a711-6071df37d10f"
      },
      "source": [
        "sequences_train[0][:5]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10660, 19698, 19699, 3, 1142]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2byxJtiw9D6U"
      },
      "source": [
        "The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message (without formatting):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6VPI41_t4x0"
      },
      "source": [
        "index_to_word = dict((i,w) for w,i in word_index.items())"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "C0wJTi_duldw",
        "outputId": "a54542d3-41c0-4d56-defa-0ee1d0425b1f"
      },
      "source": [
        "\" \".join([index_to_word[i] for i in sequences_train[0]])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"teens 'know little' of politics teenagers questioned for a survey have shown little interest in politics and have little knowledge only a quarter of 14 16 year olds knew that labour was the government the tories were the official opposition and the lib dems were the third party almost all could identify tony blair but only one in six knew who michael howard was and just one in 10 recognised charles kennedy the icm survey interviewed 110 pupils for education watchdog ofsted nearly half those pupils polled said it was not important for them to know more about what the political parties stand for and 4 of those questioned thought the conservatives were in power while 2 of them believed the lib dems were the survey also looked at issues of nationality it found the union flag and fish and chips topped the list of symbols and foods associated with being british many of the pupils also looked on themselves as english scottish or welsh rather than british while the notion of being european hardly occurred to anyone\""
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys3-057F9H84"
      },
      "source": [
        "Let's display the tokenized items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "hemZNu0cxA8N",
        "outputId": "febe1166-b1df-4f23-a43e-7eb9c65b83ab"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "length_sequences = [len(seq) for seq in sequences_train]\n",
        "plt.hist(length_sequences, bins = 50)\n",
        "plt.title('Histogram of sequence lengths');"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfjklEQVR4nO3de1SUdf4H8PcAgiaiYTJYsnY0cVErMBVQvDAwYCEKClZbqVSrnaMikrcszS662uLlyNYpTqV2ai/JBrbpBgghtql1NpFyqbxEoMHQchEkmGHGz+8Pf82COAwYOPj1/Tqnc5rvfJ/n+Xw/4HsennkYNCIiICIipTg5ugAiIup6DHciIgUx3ImIFMRwJyJSEMOdiEhBDHciIgUx3BUTFRWFY8eOOboMh8rJycHUqVMREBCA//znP44up0c4duwYpkyZ4pBjp6amYsWKFQ459s2M4X4D0el0+Oyzz1qNffDBB3j44Yetj/fv34/AwMB293Pu3DmMHDkSZrO5W+p0tC1btmDdunU4fvw4Ro0a5ehybiqOfBGh1hju1OUc/aLx448/YsSIEQ6tgcjRGO6KaXl2X1RUhNmzZ2Ps2LGYOHEi/vCHPwAAHn30UQDA+PHjERAQgOPHj+PSpUt47bXXEBoaiuDgYKxatQr19fXW/WZmZiI0NBSBgYF49dVXWx0nNTUViYmJWLFiBcaOHYuMjAwUFRXhwQcfxLhx4xASEoIXX3wRJpPJur+RI0fivffeQ0REBAICArBjxw6UlpbioYcewtixY7Fs2bJW81uyVavJZEJAQAAsFgtmzZqF8PDwNtuKCDZt2oTg4GCMHTsW0dHR+O677wAAJpMJW7ZswbRp0zBx4kSsX78eTU1N1m3ffPNNhISEICQkBOnp6Rg5ciR++OEHAMBjjz2GvXv3Wude+RPVmTNnkJCQgAkTJiAyMhIHDhywPrdmzRq88MILWLhwIQICAhAfH4/S0lLr86dOnbJuO3HiRLz++uvWPqSlpSE8PByBgYFYtmwZamtrbX9ztGAwGLB06VIEBQVBp9PhnXfesT6XmpqKZcuWYdWqVQgICEBUVBS++uor6/MnT55ETEwMAgICkJiYiKSkJGzfvh0///wzfv/736OyshIBAQEICAiAwWAAADQ3N9vcX1paGiZPnoyAgABERkbiyJEjHVoD2SF0wwgNDZV//etfrcb+/ve/y0MPPXTVOXPnzpWMjAwREbl48aIcP35cRETKysrE19dXmpubrdvt3btXwsPDpbS0VC5evCiLFy+WFStWiIjIqVOnxN/fX7744gsxGo2yefNmGTVqlPU4O3fulFGjRklOTo5YLBZpbGyUr776So4fPy7Nzc1SVlYm06dPl127dlmP5+vrK0899ZTU19fLd999J6NHj5Z58+ZJaWmp1NXVyf333y8ffPDBVfvQXq2/7LukpOSq2xYUFEhsbKxcuHBBLl26JKdPnxaDwSAiIhs3bpRFixZJTU2N1NfXy6JFiyQlJUVERA4dOiTBwcHy7bffSkNDgyQnJ7c6zqOPPirvv//+Vb8uDQ0NMmXKFElPT5fm5mY5efKkTJgwQU6dOiUiIqtXr5YJEybIiRMnpLm5WZKTkyUpKUlEROrr62XSpEny1ltvSVNTk9TX10thYaGIiOzevVvi4+OlvLxcjEajrFu3TpYvX37VdR89elQmT54sIiIWi0ViY2MlNTVVjEajlJaWik6nk4KCAuvXc8yYMZKfny9ms1lSUlIkPj5eRESMRqNMmzZNdu/eLSaTSbKysmT06NGybdu2Nsf5RXv7O3PmjEyZMkUqKipE5PL35g8//HDVNVDn8Mz9BrN48WKMGzfO+t8LL7xgc66LiwtKS0tRXV2Nvn37wt/f3+bcf/zjH1iwYAF8fHzQt29fJCcn48CBAzCbzfj4448RGhqKcePGwdXVFYmJidBoNK229/f3R3h4OJycnNC7d2+MGTMG/v7+cHFxwZAhQ/Dggw/iiy++aLXNk08+CXd3d4wYMQK+vr6YNGkSfHx80K9fP0yZMsXmm6Ht1WqPi4sLGhoacPbsWYgIhg8fDi8vL4gI3n//faxduxYDBgyAu7s7Fi1ahP379wMA/vnPf2L27Nnw9fXFLbfcgiVLltg91i/y8/Nxxx13YM6cOXBxccGoUaMQGRmJjz/+2DonPDwc99xzD1xcXDBz5kwUFxdbt73tttvw+OOPw83NDe7u7rj33nsBAH/961+xfPlyeHt7w9XVFUuWLEFWVpbdPnz11Veorq7GkiVL4OrqCh8fH8ydO7fVTxP33Xcfpk6dCmdnZ8yaNQvffPMNAODEiRMwm82YN28eevXqhYiICNx99912e2Brf87OzjCZTDhz5gyam5sxZMgQ/OY3v+lwb8k2F0cXQJ3z6quvYuLEidbHH3zwQavLAS1t3LgRO3fuxP33348hQ4ZgyZIlCA0NvercyspK3HHHHdbHd9xxB8xmM6qqqlBZWQlvb2/rc3369MGAAQNabd/yeQD4/vvvsXnzZnz99ddobGyExWLB6NGjW8257bbbrP/v5ubW5vF///vfTteq1Wqvus0vgoOD8cgjj+DFF1/E+fPnERERgdWrV8NoNKKxsRGzZ8+2zhURXLp0yXrMMWPGtDpmR50/fx5FRUUYN26cdcxisWDmzJnWxy3X3rt3b/z8888AgPLycpth9+OPP2Lx4sVwcvrfOZqTk5PdPpw/fx6VlZVt6mn5+Mp6jEYjzGYzKisrodVqW724Dx48uN31t7e/oUOHYu3atUhNTcXp06cREhKCNWvW2P06kn0Md4Xdeeed2LZtGy5duoTs7GwkJibi2LFjbc66AcDLywvnz5+3Pv7xxx/h4uKCgQMHwsvLC99//731uaampjbXdq/c54YNGzBq1Chs3boV7u7u2L17N7KysrpkXe3V2hHz5s3DvHnzUFVVhaSkJLz55ptITExE7969sX///qsGi5eXF8rLy1sds6U+ffqgsbHR+rjlC9PgwYMxfvx47Nq1q8NrbLltyzPqlry9vbFp0ybcd999nd7nkCFDkJ2d3el6Bg0aBIPBABGxfs3Ly8vh4+MDoO33QUdER0cjOjoaFy9exPr165GSkoI//vGPnd4PtcbLMgrbt28fqqur4eTkBA8PDwCXz+w8PT3h5OSEsrIy69wZM2Zgz549KCsrQ0NDA7Zv3477778fLi4uiIyMRF5eHr788kuYTCakpqZC7HxSdENDA/r27Yu+ffvizJkz+Mtf/tJl62qvVnuKiopw4sQJNDc3o0+fPnB1dYWTkxOcnJwQHx+PTZs2oaqqCsDlNx0PHz4MAJg+fToyMjJw+vRpNDY24k9/+lOr/fr5+SEnJweNjY344YcfkJ6ebn1u2rRpKCkpQWZmJpqbm9Hc3IyioiKcOXPGbr3Tpk3DTz/9hN27d8NkMuHixYs4ceIEAODhhx/Gjh07rC901dXVOHjwoN193nPPPejbty/S0tLQ1NQEi8WC7777DkVFRXa39ff3h7OzM959912YzWYcPHiw1ZujAwcORG1tbas349tz9uxZHDlyBCaTCa6urnBzc2v1kwhdO3ZRYYcPH0ZUVBQCAgKwceNGbN++Hb1790afPn3w1FNP4eGHH8a4ceNQWFiIOXPmYObMmXj00UcRFhYGV1dXrFu3DgAwYsQIrFu3DsnJyZg8eTJuueUWeHp6wtXV1eaxV69ejY8++ghjx47FunXr8MADD3TZutqr1Z6GhgY899xzmDBhAkJDQzFgwAA88cQTAICVK1di6NChmDt3LsaOHYsFCxZYf2KZOnUq5s+fj/nz50Ov1yMoKKjVfufPn49evXph4sSJWL16NaKjo63Pubu746233sKBAwcwefJkhISEICUlxebdQC25u7vj7bffxieffIJJkyYhMjLS+ktq8+bNg06nw+OPP46AgADMnTu3QwHt7OyM119/Hd988w3CwsIQFBSE5557DhcvXrS7raurK1JTU5Geno7x48fjww8/xLRp06zfC8OHD0dUVBTCw8Mxbtw4690ytphMJmzduhWBgYEICQlBdXU1kpOT7dZB9mnE3ikY0RUaGhowfvx4ZGVlWX8cvxmNHDkS2dnZGDp0qKNLcaj4+Hg89NBDmDNnjqNLoRZ45k4dkpeXh8bGRvz888/YsmULfH19MWTIEEeXRQ7w+eef46effoLZbEZGRga+/fZbTJ482dFl0RX4hip1SG5uLlatWgURwZgxY7Bt27ZrevOMbnzff/89kpKS0NjYiCFDhmDnzp3w8vJydFl0BV6WISJSEC/LEBEpqEdcliksLISbm1u7c4xGo905Nyv2pn3sj23sjW03Qm+MRqPN3zzvEeHu5uYGPz+/ducUFxfbnXOzYm/ax/7Yxt7YdiP05pePqbgaXpYhIlIQw52ISEEMdyIiBTHciYgUxHAnIlIQw52ISEEMdyIiBTHciYgUxHAnIlKQsuHe1Gzp1DgRkUp6xMcPdIfevZxx55r9bcZLNkc5oBoioutL2TN3W9o7c+dZPRGpQtkzd1tsndEDPKsnInXcdGfuREQ3A4Y7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKajD4W6xWBATE4NFixYBAMrKyhAfHw+9Xo+kpCSYTCYAgMlkQlJSEvR6PeLj43Hu3LnuqZyIiGzqcLi/8847GD58uPVxSkoKFixYgJycHHh4eCA9PR0AsHfvXnh4eCAnJwcLFixASkpK11dNRETt6lC4V1RUID8/H3FxcQAAEcHRo0cRGRkJAIiNjUVubi4AIC8vD7GxsQCAyMhIHDlyBCLSHbUTEZENLh2ZtGnTJqxcuRINDQ0AgJqaGnh4eMDF5fLm3t7eMBgMAACDwYDBgwdf3rmLC/r164eamhp4enra3L/RaERxcXG7NTQ1Ndmd05Kfn1+H57bUmWP0FJ3tzc2G/bGNvbHtRu+N3XD/5JNP4OnpiTFjxuDYsWPdUoSbm5vdMC4uLr7mwO6M63GMrna9enOjYn9sY29suxF6096Lj91w//LLL5GXl4eCggIYjUZcvHgRGzduRF1dHcxmM1xcXFBRUQGtVgsA0Gq1KC8vh7e3N8xmM+rr63Hrrbd23WqIiMguu9fcn376aRQUFCAvLw/btm1DUFAQtm7disDAQGRlZQEAMjIyoNPpAAA6nQ4ZGRkAgKysLAQFBUGj0XTjEoiI6ErXfJ/7ypUrsWvXLuj1etTW1iI+Ph4AEBcXh9raWuj1euzatQsrVqzosmKJiKhjOvSG6i8CAwMRGBgIAPDx8bHe/tiSm5sbdu7c2TXVERHRNeFvqBIRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuLfQ1Gzp1DgRUU/l4ugCepLevZxx55r9bcZLNkc5oBoiomvHM3ciIgXZPXM3Go145JFHYDKZYLFYEBkZicTERJSVlSE5ORm1tbUYPXo0XnnlFbi6usJkMmHVqlU4efIkBgwYgO3bt2PIkCHXYy1ERPT/7J65u7q6Ys+ePfjwww+RmZmJw4cPo7CwECkpKViwYAFycnLg4eGB9PR0AMDevXvh4eGBnJwcLFiwACkpKd2+CCIias1uuGs0GvTt2xcAYDabYTabodFocPToUURGRgIAYmNjkZubCwDIy8tDbGwsACAyMhJHjhyBiHRX/UREdBUdekPVYrFg9uzZKC0txe9+9zv4+PjAw8MDLi6XN/f29obBYAAAGAwGDB48+PLOXVzQr18/1NTUwNPT0+b+jUYjiouL262hqanJ7pyW/Pz8Ojy3Izpz7Outs7252bA/trE3tt3ovelQuDs7O2Pfvn2oq6vD4sWLcfbs2S4tws3NzW4YFxcXd3lgd4Yjj22Po3vT07E/trE3tt0IvWnvxadTd8t4eHggMDAQhYWFqKurg9lsBgBUVFRAq9UCALRaLcrLywFcvoxTX1+PW2+99VprJyKia2A33Kurq1FXVwfg8o8pn332GYYPH47AwEBkZWUBADIyMqDT6QAAOp0OGRkZAICsrCwEBQVBo9F0V/1ERHQVdi/LVFZWYs2aNbBYLBARTJ8+HaGhobjrrruwfPly7NixA35+foiPjwcAxMXFYeXKldDr9ejfvz+2b9/e7YsgIqLW7Ib7b3/7W2RmZrYZ9/Hxsd7+2JKbmxt27tzZNdUREdE14W+oEhEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOHeAU3Nlk6NExE5Wof+zN7NrncvZ9y5Zn+b8ZLNUQ6ohojIPp65ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECrIb7uXl5XjsscfwwAMPICoqCnv27AEA1NbWIiEhAREREUhISMCFCxcAACKCl19+GXq9HtHR0Th58mT3roCIiNqwG+7Ozs5Ys2YNDhw4gL/97W/485//jNOnTyMtLQ3BwcHIzs5GcHAw0tLSAAAFBQUoKSlBdnY2XnrpJWzYsKG710BERFewG+5eXl4YPXo0AMDd3R3Dhg2DwWBAbm4uYmJiAAAxMTE4ePAgAFjHNRoN/P39UVdXh8rKym5cAhERXcmlM5PPnTuH4uJi3HvvvaiqqoKXlxcAYNCgQaiqqgIAGAwGeHt7W7fx9vaGwWCwzr0ao9GI4uLido/d1NRkd05Lfn5+HZ77a3Smpu7S2d7cbNgf29gb22703nQ43BsaGpCYmIi1a9fC3d291XMajQYajeaai3Bzc7MbxsXFxdctsDujJ9TUU3vTU7A/trE3tt0IvWnvxadDd8s0NzcjMTER0dHRiIiIAAAMHDjQermlsrISnp6eAACtVouKigrrthUVFdBqtddcPBERdZ7dcBcRPPvssxg2bBgSEhKs4zqdDpmZmQCAzMxMhIWFtRoXERQWFqJfv37tXpIhIqKuZ/eyzL///W/s27cPvr6+mDVrFgAgOTkZCxcuRFJSEtLT03H77bdjx44dAICpU6fi0KFD0Ov16NOnDzZt2tS9KyAiojbshvu4cePw7bffXvW5X+55b0mj0eD555//9ZUREdE142+oEhEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECmK4ExEpiOH+KzQ1Wzo1TkR0vXTqb6hSa717OePONfvbjJdsjnJANURE/8MzdyIiBTHciYgUxHAnIlIQw52ISEEMdyIiBd3w4c7bDomI2rrhb4Xk7YhERG3d8GfuRETUFsOdiEhBDHciIgUx3ImIFMRwJyJSEMOdiEhBDHciIgUx3ImIFMRwJyJSEMOdiEhBDHciIgUx3ImIFMRwJyJSkN1wf+aZZxAcHIwZM2ZYx2pra5GQkICIiAgkJCTgwoULAAARwcsvvwy9Xo/o6GicPHmy+yonIiKb7Ib77Nmz8eabb7YaS0tLQ3BwMLKzsxEcHIy0tDQAQEFBAUpKSpCdnY2XXnoJGzZs6JaiiYiofXbDffz48ejfv3+rsdzcXMTExAAAYmJicPDgwVbjGo0G/v7+qKurQ2VlZTeUTURE7bmmP9ZRVVUFLy8vAMCgQYNQVVUFADAYDPD29rbO8/b2hsFgsM61xWg0ori4uN05TU1NV53j5+fX2fKvC3vr6Uq2ekOXsT+2sTe23ei9+dV/iUmj0UCj0fyqfbi5udkN6eLi4h4b5FdzPWu90XpzvbE/trE3tt0IvWnvxeea7pYZOHCg9XJLZWUlPD09AQBarRYVFRXWeRUVFdBqtddyCCIi+hWuKdx1Oh0yMzMBAJmZmQgLC2s1LiIoLCxEv3797F6SISKirmf3skxycjI+//xz1NTUYMqUKVi6dCkWLlyIpKQkpKen4/bbb8eOHTsAAFOnTsWhQ4eg1+vRp08fbNq0qdsXQEREbdkN923btl11fM+ePW3GNBoNnn/++V9f1Q2uqdmC3r2cOzxORNTVfvUbqtRW717OuHPN/jbjJZujHFANEd2M+PEDREQKYrgTESmI4U5EpCCGOxGRghjuREQKYrgTESmI4U5EpCCGOxGRghjuREQKYrgTESmI4U5EpCCGOxGRghjuREQKYrgTESmI4U5EpCCGOxGRghjuREQKYrhfR03Nli4ZJyKyh39m7zpq78/v8c/yEVFX4pk7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIY7EZGCGO5ERApiuBMRKYjhTkSkIIZ7D8ZPiySia8VPhezB2vsUSSKi9vDMnYhIQd0S7gUFBYiMjIRer0daWlp3HOKmduVlGT8/v6uOE9HNq8svy1gsFrz44ovYtWsXtFot4uLioNPpcNddd3X1oW5ati7XfPPS9KvOb2q2oHcv504dw9Y217IvIrr+ujzci4qKMHToUPj4+AAAoqKikJuby3C/Droy9LtqX1017ijt1dPT1ubInva0XvRE17sXGhGRrtzhxx9/jMOHD2Pjxo0AgMzMTBQVFWH9+vU2tyksLISbm1tXlkFEpDyj0Qh/f/+rPtcj7paxVRwREV2bLn9DVavVoqKiwvrYYDBAq9V29WGIiKgdXR7ud999N0pKSlBWVgaTyYT9+/dDp9N19WGIiKgdXX5ZxsXFBevXr8eTTz4Ji8WCOXPmYMSIEV19GCIiakeXv6FKRESOx99QJSJSEMOdiEhBPT7cb9aPMnjmmWcQHByMGTNmWMdqa2uRkJCAiIgIJCQk4MKFCwAAEcHLL78MvV6P6OhonDx50rpNRkYGIiIiEBERgYyMjOu+ju5QXl6Oxx57DA888ACioqKwZ88eAOwPcPm+57i4OMycORNRUVHYuXMnAKCsrAzx8fHQ6/VISkqCyWQCAJhMJiQlJUGv1yM+Ph7nzp2z7uuNN96AXq9HZGQkDh8+7JD1dAeLxYKYmBgsWrQIgMK9kR7MbDZLWFiYlJaWitFolOjoaDl16pSjy7ouPv/8c/n6668lKirKOrZlyxZ54403RETkjTfekFdeeUVERPLz8+WJJ56QS5cuyfHjxyUuLk5ERGpqakSn00lNTY3U1taKTqeT2tra67+YLmYwGOTrr78WEZH6+nqJiIiQU6dOsT8icunSJbl48aKIiJhMJomLi5Pjx49LYmKifPTRRyIism7dOnnvvfdEROTdd9+VdevWiYjIRx99JMuWLRMRkVOnTkl0dLQYjUYpLS2VsLAwMZvNDlhR13v77bclOTlZFi5cKCKibG969Jl7y48ycHV1tX6Uwc1g/Pjx6N+/f6ux3NxcxMTEAABiYmJw8ODBVuMajQb+/v6oq6tDZWUlPv30U0yaNAkDBgxA//79MWnSpJ57ltEJXl5eGD16NADA3d0dw4YNg8FgYH8AaDQa9O3bFwBgNpthNpuh0Whw9OhRREZGAgBiY2Ot/47y8vIQGxsLAIiMjMSRI0cgIsjNzUVUVBRcXV3h4+ODoUOHoqioyDGL6kIVFRXIz89HXFwcgMs/1anamx4d7gaDAd7e3tbHWq0WBoPBgRU5VlVVFby8vAAAgwYNQlVVFYC2ffL29obBYLgp+nfu3DkUFxfj3nvvZX/+n8ViwaxZszBx4kRMnDgRPj4+8PDwgIvL5Tuff1k/cLk3gwcPBnD5NuZ+/fqhpqZG2d5s2rQJK1euhJPT5eirqalRtjc9OtzJNo1GA41G4+gyHKqhoQGJiYlYu3Yt3N3dWz13M/fH2dkZ+/btw6FDh1BUVISzZ886uqQe4ZNPPoGnpyfGjBnj6FKuix4d7vwog9YGDhyIyspKAEBlZSU8PT0BtO1TRUUFtFqt0v1rbm5GYmIioqOjERERAYD9uZKHhwcCAwNRWFiIuro6mM1mAP9bP3C5N+Xl5QAuX8apr6/HrbfeqmRvvvzyS+Tl5UGn0yE5ORlHjx7Fxo0ble1Njw53fpRBazqdDpmZmQAuf9pmWFhYq3ERQWFhIfr16wcvLy+EhITg008/xYULF3DhwgV8+umnCAkJceQSuoSI4Nlnn8WwYcOQkJBgHWd/gOrqatTV1QEAmpqa8Nlnn2H48OEIDAxEVlYWgMt3CP3y70in01nvEsrKykJQUBA0Gg10Oh32798Pk8mEsrIylJSU4J577nHMorrI008/jYKCAuTl5WHbtm0ICgrC1q1b1e2NI9/N7Yj8/HyJiIiQsLAwee211xxdznWzfPlymTRpkowaNUomT54s77//vlRXV8u8efNEr9fL/PnzpaamRkQu3yGxYcMGCQsLkxkzZkhRUZF1P3v37pXw8HAJDw+X9PR0Ry2nS33xxRfi6+srM2bMkJkzZ8rMmTMlPz+f/RGR4uJimTVrlsyYMUOioqIkNTVVRERKS0tlzpw5Eh4eLkuXLhWj0SgiIk1NTbJ06VIJDw+XOXPmSGlpqXVfr732moSFhUlERITk5+c7ZD3d5ejRo9a7ZVTtDT9+gIhIQT36sgwREV0bhjsRkYIY7kRECmK4ExEpiOFORKQghjsRkYIY7kRECvo/TsKBP2RepxwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxKfDQdK9Naq"
      },
      "source": [
        "We truncate and pad the sequences to 1000 words max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaPksLobv3QQ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 1000\n",
        "x_train = pad_sequences(maxlen = max_length, sequences = sequences_train)\n",
        "x_test = pad_sequences(maxlen = max_length, sequences = sequences_test)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47JcsIJ5xo6t"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical \n",
        "\n",
        "y_train = to_categorical(targets_train)\n",
        "y_test = to_categorical(targets_test)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CILokDBQ9Ycw"
      },
      "source": [
        "### A simple supervised CBOW model in Keras\n",
        "\n",
        "The following computes a very simple model, as described in [fastText](https://github.com/facebookresearch/fastText):\n",
        "\n",
        "\n",
        "- Build an embedding layer mapping each word to a vector representation\n",
        "- Compute the vector representation of all words in each sequence and average them\n",
        "- Add a dense layer to output 20 classes (+ softmax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M7lzyOOyLN_"
      },
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM \n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, Input\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml9y9nR0y_Ty",
        "outputId": "aa6ed157-eac8-45c9-b278-859254e01ef5"
      },
      "source": [
        "emb_size = 50\n",
        "n_classes = len(target_names)\n",
        "\n",
        "inputs = Input(shape = (max_length,))\n",
        "embedding = Embedding(input_dim = MOST_FREQUENT, output_dim = emb_size)(inputs)\n",
        "avgpool = GlobalAveragePooling1D()(embedding)\n",
        "predictions = Dense(units = n_classes, activation = 'softmax')(avgpool)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = predictions)\n",
        "model.compile(optimizer = Adam(learning_rate = 0.01),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['Accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 1000)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 1000, 50)          1000000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_6 ( (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 255       \n",
            "=================================================================\n",
            "Total params: 1,000,255\n",
            "Trainable params: 1,000,255\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp8XctyMzu0Y",
        "outputId": "89211e9d-7818-41fb-9770-c250d2f5f457"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 32, \n",
        "          epochs = 10, validation_split = 0.1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 2s 23ms/step - loss: 1.4084 - Accuracy: 0.4665 - val_loss: 1.0473 - val_Accuracy: 0.7734\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.6674 - Accuracy: 0.9061 - val_loss: 0.4436 - val_Accuracy: 0.9360\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 1s 20ms/step - loss: 0.2337 - Accuracy: 0.9824 - val_loss: 0.2696 - val_Accuracy: 0.9655\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.1089 - Accuracy: 0.9934 - val_loss: 0.2070 - val_Accuracy: 0.9704\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 1s 20ms/step - loss: 0.0605 - Accuracy: 0.9967 - val_loss: 0.1865 - val_Accuracy: 0.9704\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.0383 - Accuracy: 0.9995 - val_loss: 0.1709 - val_Accuracy: 0.9704\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 1s 20ms/step - loss: 0.0257 - Accuracy: 1.0000 - val_loss: 0.1694 - val_Accuracy: 0.9704\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 1s 20ms/step - loss: 0.0180 - Accuracy: 1.0000 - val_loss: 0.1538 - val_Accuracy: 0.9704\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.0134 - Accuracy: 1.0000 - val_loss: 0.1557 - val_Accuracy: 0.9704\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 1s 20ms/step - loss: 0.0105 - Accuracy: 1.0000 - val_loss: 0.1495 - val_Accuracy: 0.9704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd00368a710>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3b6U-KM0_5m",
        "outputId": "0736831f-2a1c-42a0-e34a-62ef07d9f91c"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1229 - Accuracy: 0.9550\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12291813641786575, 0.9549999833106995]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYagD9l64_bS"
      },
      "source": [
        "### Building more complex models\n",
        "\n",
        "- From the previous template, we build more complex models using:\n",
        "  - **1d convolution and 1d maxpooling**. Note that we will still need a GloabalAveragePooling or Flatten after the convolutions as the final `Dense` layer expects a fixed size input;\n",
        "  - **Recurrent neural networks through LSTM** (we will need to **reduce sequence length before using the LSTM layer**).\n",
        "\n",
        "- We may try different architectures with:\n",
        "  - more intermediate layers, combination of dense, conv, recurrent\n",
        "  - different recurrent (GRU, RNN)\n",
        "  - bidirectional LSTMs\n",
        "\n",
        "**Note**: The goal is to build working models rather than getting better test accuracy as this task is already very well solved by the simple model.  Build your model, and verify that they converge to OK results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZukrH5t5Atg"
      },
      "source": [
        "max_length = 500\n",
        "\n",
        "x_train = pad_sequences(maxlen = max_length, sequences = sequences_train)\n",
        "x_test = pad_sequences(maxlen = max_length, sequences = sequences_test)\n",
        "\n",
        "y_train = to_categorical(targets_train)\n",
        "y_test = to_categorical(targets_test)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlY3cGOq1qBc",
        "outputId": "48eda4c9-13c3-4121-b457-f8cf8615854c"
      },
      "source": [
        "Recc_units = 16\n",
        "\n",
        "inputs = Input(shape = (max_length,))\n",
        "embedding = Embedding(input_dim = MOST_FREQUENT, output_dim = emb_size)(inputs)\n",
        "\n",
        "recurrent = LSTM(units = Recc_units )(embedding)\n",
        "predictions = Dense(units = n_classes, activation = 'softmax')(recurrent)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = predictions)\n",
        "model.compile(optimizer = Adam(learning_rate = 0.01),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['Accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 500)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 500, 50)           1000000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 16)                4288      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 85        \n",
            "=================================================================\n",
            "Total params: 1,004,373\n",
            "Trainable params: 1,004,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SbnUDZp1tXq",
        "outputId": "64cd3686-2421-4b5c-9326-cc9e880aff66"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 32, \n",
        "          epochs = 10, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 15s 227ms/step - loss: 1.2471 - Accuracy: 0.4748 - val_loss: 0.8477 - val_Accuracy: 0.6798\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 13s 221ms/step - loss: 0.4062 - Accuracy: 0.8930 - val_loss: 0.7021 - val_Accuracy: 0.7783\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 17s 300ms/step - loss: 0.1184 - Accuracy: 0.9698 - val_loss: 0.8230 - val_Accuracy: 0.7685\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 13s 234ms/step - loss: 0.0354 - Accuracy: 0.9912 - val_loss: 0.7113 - val_Accuracy: 0.8079\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 16s 276ms/step - loss: 0.0083 - Accuracy: 0.9989 - val_loss: 0.8797 - val_Accuracy: 0.7635\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 13s 233ms/step - loss: 0.0217 - Accuracy: 0.9951 - val_loss: 0.9305 - val_Accuracy: 0.7488\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 13s 220ms/step - loss: 0.0067 - Accuracy: 0.9989 - val_loss: 1.0232 - val_Accuracy: 0.7586\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 12s 219ms/step - loss: 0.0022 - Accuracy: 1.0000 - val_loss: 0.8861 - val_Accuracy: 0.7931\n",
            "Epoch 9/10\n",
            "29/57 [==============>...............] - ETA: 5s - loss: 0.0016 - Accuracy: 1.0000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0yJB6iQ7dig"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHiJZeHy7DKy"
      },
      "source": [
        "\n",
        "### Add more Recurrent units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ3yZ5dA41aW"
      },
      "source": [
        "Recc_units = 32\n",
        "\n",
        "inputs = Input(shape = (max_length,))\n",
        "embedding = Embedding(input_dim = MOST_FREQUENT, output_dim = emb_size)(inputs)\n",
        "\n",
        "recurrent = LSTM(units = Recc_units)(embedding)\n",
        "\n",
        "predictions = Dense(units = n_classes, activation = 'softmax')(recurrent)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = predictions)\n",
        "model.compile(optimizer = Adam(learning_rate = 0.001),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['Accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7q3DQhB6Ty7"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 32, \n",
        "          epochs = 10, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANkNZ5Dc7M0X"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3B7aVNMEZ9d"
      },
      "source": [
        "### Loading pre-trained embeddings\n",
        "\n",
        "The file `glove100K.100d.txt` is an extract of [Glove](http://nlp.stanford.edu/projects/glove/) Vectors, that were trained on english Wikipedia 2014 + Gigaword 5 (6B tokens).\n",
        "\n",
        "We extracted the `100 000` most frequent words. They have a dimension of `100`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XaA8mmQ7f1U"
      },
      "source": [
        "# Get pretrained Glove Word2Vec\n",
        "URL_REPRESENTATIONS = \"https://github.com/Badisj/Recurrent-Neural-Nets-NLP/releases/download/v1.0/glove100K.100d.txt.zip\"\n",
        "ZIP_REPRESENTATIONS = \"glove100k.100d.zip\"\n",
        "FILE_REPRESENTATIONS = \"glove100K.100d.txt\"\n",
        "\n",
        "if not op.exists(ZIP_REPRESENTATIONS):\n",
        "    print('Downloading from %s to %s...' % (URL_REPRESENTATIONS, ZIP_REPRESENTATIONS))\n",
        "    urlretrieve(URL_REPRESENTATIONS, './' + ZIP_REPRESENTATIONS)\n",
        "    \n",
        "if not op.exists(FILE_REPRESENTATIONS):\n",
        "    print(\"extracting %s...\" % ZIP_REPRESENTATIONS)\n",
        "    myzip = zipfile.ZipFile(ZIP_REPRESENTATIONS)\n",
        "    myzip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DASzqkcVLH6e"
      },
      "source": [
        "embeddings_index = {}\n",
        "embeddings_vectors = []\n",
        "with open('glove100K.100d.txt', 'rb') as f:\n",
        "    word_idx = 0\n",
        "    for line in f:\n",
        "        values = line.decode('utf-8').split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = word_idx\n",
        "        embeddings_vectors.append(vector)\n",
        "        word_idx = word_idx + 1\n",
        "\n",
        "inv_index = {v: k for k, v in embeddings_index.items()}\n",
        "print(\"found %d different words in the file\" % word_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF1X49NSNWy2"
      },
      "source": [
        "# Stack all embeddings in a large numpy array\n",
        "glove_embeddings = np.vstack(embeddings_vectors)\n",
        "glove_norms = np.linalg.norm(glove_embeddings, axis=-1, keepdims=True)\n",
        "glove_embeddings_normed = glove_embeddings / glove_norms\n",
        "print(glove_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7xesrDDNXau"
      },
      "source": [
        "def get_emb(word):\n",
        "    idx = embeddings_index.get(word)\n",
        "    if idx is None:\n",
        "        return None\n",
        "    else:\n",
        "        return glove_embeddings[idx]\n",
        "\n",
        "    \n",
        "def get_normed_emb(word):\n",
        "    idx = embeddings_index.get(word)\n",
        "    if idx is None:\n",
        "        return None\n",
        "    else:\n",
        "        return glove_embeddings_normed[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJrO5wWtNXjc"
      },
      "source": [
        "get_emb(\"computer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG_37tt1OXBX"
      },
      "source": [
        "### Get similar words to a query "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIdEqH5fNXt4"
      },
      "source": [
        "def get_similar(query, embedding_matrix, top = 10):\n",
        "    emb_query = get_emb(query)\n",
        "    dist = []\n",
        "    for i in range(embedding_matrix.shape[0]):\n",
        "        dist.append(np.linalg.norm(emb_query - embedding_matrix[i,:]))\n",
        "    idx = np.argsort(np.array(dist))[1:top+1]\n",
        "    \n",
        "    return [inv_index[id] for id in idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsmaTRjdQCJv"
      },
      "source": [
        "get_similar('computer', glove_embeddings, top = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhvEsRVLSOtL"
      },
      "source": [
        "### Displaying words with TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrUf_bVpRNNF"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "embedding_tsne = TSNE(n_components = 2, perplexity = 30).fit_transform(glove_embeddings_normed[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWw-bg_BSUMj"
      },
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "axis = plt.gca()\n",
        "np.set_printoptions(suppress=True)\n",
        "plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1], marker = \".\", s = 1)\n",
        "\n",
        "for idx in range(1000):\n",
        "    plt.annotate(inv_index[idx],\n",
        "                 xy = (embedding_tsne[idx, 0], embedding_tsne[idx, 1]),\n",
        "                 xytext = (0, 0), textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tsne.png\")\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCAxTg1XUk_i"
      },
      "source": [
        "### Using pre-trained embeddings in our model\n",
        "\n",
        "We want to use these pre-trained embeddings for transfer learning. This process is rather similar than transfer learning in image recognition: the features learnt on words might help us bootstrap the learning process, and increase performance if we don't have enough training data.\n",
        "- We initialize embedding matrix from the model with Glove embeddings:\n",
        " - take all unique words from our BBC news dataset to build a vocabulary (`MAX_NB_WORDS = 20000`), and look up their Glove embedding \n",
        " - place the Glove embedding at the corresponding index in the matrix\n",
        " - if the word is not in the Glove vocabulary, we only place zeros in the matrix\n",
        "- We may fix these embeddings or fine-tune them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTnrchAbTU8Y"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# prepare embedding matrix\n",
        "nb_words_in_matrix = 0\n",
        "nb_words = min(MOST_FREQUENT, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i >= MOST_FREQUENT:\n",
        "        continue\n",
        "    embedding_vector = get_emb(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
        "        \n",
        "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEVDFAeUV_-F"
      },
      "source": [
        "Build a custom embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M45QsVlMVNJP"
      },
      "source": [
        "Custom_embedding = Embedding(input_dim = MOST_FREQUENT, output_dim = EMBEDDING_DIM,\n",
        "                             weights = [embedding_matrix])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcRLSX2W4G5"
      },
      "source": [
        "### A model with pre-trained Embeddings\n",
        "\n",
        "Average word embeddings pre-trained with Glove / Word2Vec usually works surprisingly well. However, when averaging more than `10-15` words, the resulting vector becomes too noisy and classification performance is degraded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EtC_ApPVNve"
      },
      "source": [
        "recurrent = False\n",
        "\n",
        "inputs = Input(shape = (max_length,))\n",
        "\n",
        "embedding = Custom_embedding(inputs)\n",
        "if recurrent == True:\n",
        "    recurrent = LSTM(128)(embedding)\n",
        "    predictions = Dense(units = n_classes, activation = 'softmax')(recurrent)\n",
        "else:\n",
        "    avgpool = GlobalAveragePooling1D()(embedding)\n",
        "    predictions = Dense(units = n_classes, activation = 'softmax')(avgpool)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = predictions)\n",
        "\n",
        "# No fine tuning\n",
        "model.layers[1].trainable = False\n",
        "\n",
        "model.compile(optimizer = Adam(learning_rate = 0.01),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['Accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3doRsVJXExW"
      },
      "source": [
        "model.fit(x_train, y_train, \n",
        "          batch_size = 32, epochs = 10,\n",
        "          validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzJRP4r9X_hG"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvd3k4GrZeRE"
      },
      "source": [
        "**To conclude**\n",
        "- On this type of task, using pre-trained embeddings can degrade results as we train much less parameters and we average a large number pre-trained embeddings.\n",
        "\n",
        "- Pre-trained embeddings followed by global averaging prevents overfitting but can also cause some underfitting.\n",
        "\n",
        "- Using convolutions / LSTM should help counter the underfitting effect.\n",
        "\n",
        "- It is also advisable to treat separately pre-trained embeddings and words out of vocabulary.\n",
        "\n",
        "Pre-trained embeddings can be very useful when the training set is small and the individual text documents to classify are short: in this case there might be a single very important word in a test document that drives the label. If that word has never been seen in the training set but some synonyms were seen, the semantic similarity captured by the embedding will allow the model to generalized out of the restricted training set vocabulary.\n",
        "\n",
        "We did not observe this effect here because the document are long enough so that guessing the topic can be done redundantly. Shortening the documents to make the task more difficult could possibly highlight this benefit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY0jmE-PaO7a"
      },
      "source": [
        "## Going further\n",
        "\n",
        "- Compare pre-trained embeddings vs specifically trained embeddings\n",
        "- Train your own wordvectors in any language using [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "- Install fastText (Linux or macOS only, use the Linux VM if under Windows) and give it a try on the classification example in its repository.\n",
        "- Today, the **state-of-the-art text classification** can be achieved by **transfer learning from a language model** instead of using traditional word embeddings. See for instance: [ULMFit, Fine-tuned Language Models for Text Classification](https://arxiv.org/abs/1801.06146), [ELMO](https://allennlp.org/elmo), [GPT](https://blog.openai.com/language-unsupervised/), [BERT](https://arxiv.org/abs/1810.04805), [GPT-2](https://github.com/openai/gpt-2)."
      ]
    }
  ]
}